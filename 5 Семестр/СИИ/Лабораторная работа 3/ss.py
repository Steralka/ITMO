import numpy as np                 # импортируем библиотеку NumPy, она добавляет возможность работы с многомерными массивами (ndarray),
                                   # проводить линейную алгебру (например, умножение матриц, псевдообратная матрица, суммирование и т.д.).
                                   # используем как основу для всех численных операций.

import pandas as pd                # импортируем библиотеку pandas — она нужна для работы с табличными данными (DataFrame),
                                   # которые удобны для анализа, фильтрации, статистики и преобразований (например, нормализации).
                                   # pandas внутри хранит данные на базе numpy, но с метками столбцов и строк.

import matplotlib.pyplot as plt    # matplotlib.pyplot — это модуль для построения графиков.
                                   # предоставляет функции вроде plt.plot(), plt.bar(), plt.hist(), plt.show().
                                   # все графики из лабораторной (гистограммы, scatter и т.д.) строятся с помощью этого модуля.

from prettytable import PrettyTable # импортируем класс PrettyTable — библиотека для форматирования вывода в консоли.
                                   # позволяет создать “ASCII-таблицу” с рамками, выровненными колонками, чтобы красиво показывать статистику.

# ---------- 1) Загрузка ----------
df = pd.read_csv("california_housing_train.csv")  # pd.read_csv читает CSV-файл (таблицу, где данные разделены запятыми).
                                                  # "df" (dataframe) — объект pandas, содержащий строки и столбцы.
                                                  # pandas автоматически анализирует первую строку и использует её как имена колонок.
                                                  # каждый столбец внутри df — это Series (одномерный массив с метками).
                                                  # теперь df содержит таблицу с признаками (latitude, longitude, income, и т.д.) и целевой колонкой median_house_value.

# ---------- Базовая статистика ----------
quantiles = df.quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])  # метод .quantile() вычисляет значения для указанных квантилей.
                                                                    # внутри pandas для каждого числового столбца сортируется значение, 
                                                                    # после чего берется элемент на позиции, соответствующей проценту (например, 0.25 → 25%).
                                                                    # возвращается новый DataFrame, где индекс — это список квантилей, 
                                                                    # а колонки — исходные признаки. 
                                                                    # здесь мы берем квантили: 1%, 5%, 25%, медиану, 75%, 95%, 99%.
                                                                    # quantiles.shape ≈ (7 строк × N признаков)

stats_basic = df.describe().T                                       # describe() вычисляет базовую статистику (count, mean, std, min, 25%, 50%, 75%, max)
                                                                    # .T транспонирует таблицу, чтобы каждая строка представляла один признак, а столбцы — показатели.
                                                                    # теперь строки — названия признаков, столбцы — статистические параметры.

stats_full = stats_basic.join(quantiles.T, how="left", rsuffix="_q") # объединяем базовую таблицу stats_basic с квантили (quantiles.T),
                                                                    # чтобы собрать всё в одной таблице. 
                                                                    # join работает по индексам (у нас совпадает индекс — это имена признаков).
                                                                    # rsuffix="_q" добавляет суффикс "_q" к столбцам из правой таблицы, 
                                                                    # если совпадают названия (например, "50%" и "50%_q").

print("Статистика по признакам")                                     

table = PrettyTable()                                               
table.field_names = ["Признак", "count", "mean", "std", "min", "25%", "50%", "75%", "max"]  

for name, row in stats_basic.iterrows():                             # итерируемся по строкам DataFrame `stats_basic`.
                                                                    # iterrows() возвращает пару (index, Series), где
                                                                    # index = имя признака (например, “longitude”),
                                                                    # Series = строка со статистикой по этому признаку (mean, std и т.д.)
    
    table.add_row([                                                  # добавляем новую строку в таблицу PrettyTable.
        name,                                                        # первый элемент — название признака (index)
        f"{row['count']:.0f}",                                       # значение столбца “count”, форматируем без дробной части
        f"{row['mean']:.2f}",                                        # среднее (mean), округлено до 2 знаков после запятой
        f"{row['std']:.2f}",                                         # стандартное отклонение (std)
        f"{row['min']:.2f}",                                         # минимум
        f"{row['25%']:.2f}",                                         # нижний квартиль (25%)
        f"{row['50%']:.2f}",                                         # медиана (50%)
        f"{row['75%']:.2f}",                                         # верхний квартиль (75%)
        f"{row['max']:.2f}"                                          # максимум
    ])
print(table)                                                         # после завершения цикла печатаем таблицу.
                                                                    # метод PrettyTable форматирует всё с рамками и выравниванием.


# ---------- 2) Min–Max нормализация ----------
y_name = "median_house_value"                                        # указываем имя целевого признака, который хотим предсказывать (цена дома).
y = df[y_name].values.astype(float)                                  # извлекаем столбец как массив numpy (values), чтобы можно было использовать в формулах.
                                                                    # .astype(float) — явно преобразуем тип данных в числовой (иногда pandas оставляет float64 по умолчанию).

X_df = df.drop(columns=[y_name])                                     # создаём новый DataFrame X_df, исключая целевую колонку.
                                                                    # теперь X_df содержит только независимые признаки (факторы).

X_min = X_df.min()                                                   # вычисляем минимальное значение для каждого признака.
                                                                    # pandas делает это по каждому столбцу независимо.
                                                                    # результат: Series, где индекс — имя признака, значение — минимум.
X_max = X_df.max()                                                   # аналогично — Series с максимальными значениями по каждому признаку.

X_norm = (X_df - X_min) / (X_max - X_min)                            # выполняем min–max нормализацию.
                                                                    # pandas вычитает Series X_min из каждой строки (broadcasting по индексам),
                                                                    # затем делит поэлементно на разницу (X_max - X_min).
                                                                    # результат: все значения каждого признака масштабируются в диапазон [0, 1].
                                                                    # если где-то X_min == X_max, результат будет NaN.

X_norm = X_norm.fillna(0)                                            # заменяем NaN на 0 (например, если столбец константный).
                                                                    # теперь X_norm — нормализованный DataFrame, готовый к использованию в модели.

print("Нормализации")                                                # заголовок вывода для примера нормализации.

for col in ["longitude", "latitude", "housing_median_age", "median_income"]:  
                                                                    # выбираем 4 признака для демонстрации “до/после” нормализации.

    t = PrettyTable()                                                # создаём новую таблицу PrettyTable для каждого признака.
    t.field_names = [f"Признак: {col}", "До нормализации", "После (0–1)"]  
                                                                    # задаём заголовки — имя признака и две колонки для сравнения значений.

    for i in range(5):                                               # берём первые 5 строк исходных данных (для краткости)
        t.add_row([i+1,                                              # добавляем строку с индексом (1, 2, 3, 4, 5)
                   f"{X_df[col].iloc[i]:.2f}",                       # значение признака до нормализации, округлённое до 2 знаков
                   f"{X_norm[col].iloc[i]:.4f}"])                    # значение признака после нормализации, 4 знака после запятой
    print(t)                                                         # выводим таблицу для данного признака

# ---------- 3) Разделение train/test ----------
rng = np.random.default_rng(42)                                      # создаём генератор случайных чисел NumPy с фиксированным "seed" = 42.
                                                                    # этот seed делает результаты воспроизводимыми:
                                                                    # при каждом запуске программа создаст одно и то же случайное разделение.
                                                                    # new Generator API в NumPy (вместо np.random.seed + np.random) даёт более надёжное поведение.

idx = np.arange(len(df))                                             # создаём массив целых чисел от 0 до N-1, где N = количество строк в df.
                                                                    # np.arange возвращает одномерный массив numpy.ndarray.
                                                                    # пример: если df имеет 17000 строк, idx = [0, 1, 2, ..., 16999].

rng.shuffle(idx)                                                     # перемешиваем массив idx случайным образом "на месте".
                                                                    # теперь порядок индексов перемешан: например [132, 501, 5, 14999, ...].
                                                                    # важно: данные в df не меняются, мы просто создаём порядок для выборки.

test_ratio = 0.2                                                     # задаём долю тестовой выборки — 20%.
test_size = int(len(idx) * test_ratio)                               # вычисляем размер тестовой выборки:
                                                                    # 0.2 * 17000 = 3400.
                                                                    # преобразуем в целое число int() (чтобы не получить float).

test_idx = idx[:test_size]                                           # берём первые 3400 индексов из перемешанного списка для тестовой выборки.
train_idx = idx[test_size:]                                          # оставшиеся 13600 индексов пойдут в обучающую выборку.
                                                                    # таким образом train/test не пересекаются, и оба набора случайны.

X = X_norm.values                                                    # извлекаем значения из DataFrame X_norm как чистый NumPy массив (двумерный).
                                                                    # теперь X — это матрица размером [N, M], где N — число образцов, M — число признаков.

X_train, X_test = X[train_idx], X[test_idx]                          # создаём обучающую и тестовую матрицы признаков с помощью индексов.
                                                                    # индексация NumPy массивов по массивам индексов извлекает соответствующие строки.
                                                                    # пример: X_train.shape ≈ (13600, 8), X_test.shape ≈ (3400, 8).

y_train, y_test = y[train_idx], y[test_idx]                          # аналогично делим целевой вектор y на обучающую и тестовую части.
                                                                    # теперь у нас X_train, y_train → обучение;
                                                                    # X_test, y_test → проверка точности.
                                                                    
def add_intercept(X):
    return np.hstack([np.ones((X.shape[0], 1)), X])                  # np.ones создаёт столбец из 1 размером [N,1].
                                                                    # np.hstack() склеивает этот столбец слева к матрице X (по горизонтали).
                                                                    # результат: новая матрица X1 размером [N, M+1], где первый столбец — константа.
                                                                    # нужна, чтобы модель могла подбирать не только углы (β1, β2...), но и смещение β0.

def ols_fit(X, y):                                                   # функция "обучения" линейной регрессии (метод наименьших квадратов)
    return np.linalg.pinv(add_intercept(X)) @ y                      # формула для оценки коэффициентов β:
                                                                    # β = (X⁺) * y, где X⁺ — псевдообратная матрица (Moore-Penrose inverse).
                                                                    # np.linalg.pinv() вычисляет псевдообратную с помощью SVD (устойчиво к вырожденным матрицам).
                                                                    # затем выполняется матричное умножение @.
                                                                    # возвращаем β в виде вектора длиной M+1 (включая свободный член).

def ols_predict(X, beta):                                           # функция для предсказаний на новых данных.
    return add_intercept(X) @ beta                                  # снова добавляем столбец из 1 и умножаем на вектор β.
                                                                    # @ — это матричное произведение (np.dot).
                                                                    # результат — вектор предсказанных ŷ (одномерный массив).

def r2_score(y_true, y_pred):                                        # реализуем свою метрику качества (коэффициент детерминации R²)
    ss_res = np.sum((y_true - y_pred)**2)                            # SSR (sum of squared residuals) — сумма квадратов ошибок модели.
                                                                    # каждое отклонение (y_i - ŷ_i) возводится в квадрат, затем суммируется.

    ss_tot = np.sum((y_true - y_true.mean())**2)                     # SST (total sum of squares) — дисперсия исходных значений.
                                                                    # разность между каждым значением и средним по выборке в квадрате.

    return 1 - ss_res / ss_tot if ss_tot != 0 else 0.0               # формула R² = 1 - SSR/SST.
                                                                    # если SST=0 (все y одинаковые), возвращаем 0, чтобы избежать деления на ноль.


all_cols = list(X_df.columns)                                        # получаем список всех названий признаков в исходном наборе X_df.
                                                                    # он нужен, чтобы по имени потом найти индекс признака в матрице X.
                                                                    # пример: all_cols = ["longitude", "latitude", ..., "median_income"]

m1_cols = ["median_income"]                                          # для первой модели берём только доход.
m2_cols = ["longitude", "latitude", "housing_median_age", "median_income"]
                                                                    # для второй — географические координаты, возраст и доход.

m1_idx = [all_cols.index(c) for c in m1_cols]                        # для каждой колонки из m1_cols ищем её индекс в all_cols.
m2_idx = [all_cols.index(c) for c in m2_cols]                        # то же для второй модели.
                                                                    # так получаем позиции нужных признаков в numpy-массиве X.

df_eng = df.copy()                                                   # создаём копию исходного DataFrame, чтобы не изменять df напрямую.
                                                                    # дальше в неё добавим синтетические признаки.

df_eng["rooms_per_household"] = df_eng["total_rooms"] / df_eng["households"].replace(0, np.nan)
                                                                    # создаём новый столбец: количество комнат на одно домохозяйство.
                                                                    # сначала делим total_rooms на households.
                                                                    # .replace(0, np.nan) предотвращает деление на ноль (чтобы не получить inf).
                                                                    # результат — float с NaN там, где было деление на 0.

df_eng["bedrooms_per_room"] = df_eng["total_bedrooms"] / df_eng["total_rooms"].replace(0, np.nan)
                                                                    # второй синтетический признак — доля спален от всех комнат (характеристика жилья).
                                                                    # деление аналогично защищено заменой нулей на NaN.

df_eng["population_per_household"] = df_eng["population"] / df_eng["households"].replace(0, np.nan)
                                                                    # третий синтетический признак — плотность населения в домохозяйстве.
                                                                    # тоже защищаемся от деления на 0.

df_eng = df_eng.fillna(0)                                            # заменяем все NaN, возникшие при делении, на 0.
                                                                    # теперь df_eng полностью готов к нормализации.

y3 = df_eng[y_name].values.astype(float)                             # выделяем целевую переменную (цены домов) для модели 3.
X3_df = df_eng.drop(columns=[y_name])                                # оставляем только признаки (включая синтетические).

X3_min = X3_df.min()                                                 # вычисляем минимальные значения по каждому признаку в df_eng.
X3_max = X3_df.max()                                                 # вычисляем максимальные значения.
X3_norm = (X3_df - X3_min) / (X3_max - X3_min)                       # применяем min–max нормализацию (приводим к диапазону 0–1).
X3_norm = X3_norm.fillna(0)                                          # заменяем возможные NaN на 0 (в случае константных признаков).

X3 = X3_norm.values                                                  # извлекаем из DataFrame только числовые значения в виде numpy-массива.
X3_train, X3_test = X3[train_idx], X3[test_idx]                      # разделяем на train и test по тем же индексам, что и раньше.
y3_train, y3_test = y3[train_idx], y3[test_idx]                      # аналогично для целевой переменной.

beta1 = ols_fit(X_train[:, m1_idx], y_train)                         # обучаем первую модель (M1):
                                                                    # X_train[:, m1_idx] — извлекаем из обучающего набора только нужные колонки (доход).
                                                                    # внутри ols_fit() → добавляется столбец из 1, затем рассчитываются коэффициенты β.
                                                                    # результат beta1 — это NumPy-вектор длиной (1 + кол-во признаков), т.е. [β0, β1].

beta2 = ols_fit(X_train[:, m2_idx], y_train)                         # обучаем вторую модель (M2) с 4 признаками: долгота, широта, возраст, доход.
                                                                    # также вычисляются коэффициенты β (вектор длиной 5).

beta3 = ols_fit(X3_train, y3_train)                                  # обучаем третью модель (M3), использующую все признаки + синтетические.
                                                                    # X3_train содержит уже нормализованные данные, возможно десятки столбцов.
                                                                    # np.linalg.pinv() возвращает β — оптимальные веса, минимизирующие сумму квадратов ошибок.
y1_te = ols_predict(X_test[:, m1_idx], beta1)                        # предсказываем цены домов для тестового набора в модели M1.
                                                                    # внутри ols_predict добавляется единичный столбец, затем умножение на β.
                                                                    # результат y1_te — NumPy-вектор предсказанных значений длиной len(X_test).

y2_te = ols_predict(X_test[:, m2_idx], beta2)                        # то же для модели M2.
y3_te = ols_predict(X3_test, beta3)                                  # и для модели M3, которая имеет собственную матрицу признаков X3_test.

r2_1 = r2_score(y_test, y1_te)                                       # оцениваем качество модели M1:
                                                                    # вычисляем R² = 1 - SSR/SST.
                                                                    # возвращается скаляр (float) от 0 до 1.
r2_2 = r2_score(y_test, y2_te)                                       # то же для M2.
r2_3 = r2_score(y3_test, y3_te)                                      # и для M3.

print("📈 Сравнение трёх моделей (Min–Max нормализация)")             # просто заголовок для визуального разделения.
compare = PrettyTable()                                              # создаём новую таблицу PrettyTable.
compare.field_names = ["Модель", "Признаки", "R² (test)"]            # задаём названия колонок в таблице.

compare.add_row(["M1", "median_income", f"{r2_1:.4f}"])              # добавляем строку с результатами модели 1.
                                                                    # f"{r2_1:.4f}" форматирует число с 4 знаками после запятой.

compare.add_row(["M2", "geo + age + income", f"{r2_2:.4f}"])         # аналогично добавляем строку для модели 2.
compare.add_row(["M3", "все + синтетика", f"{r2_3:.4f}"])            # и для модели 3.

print(compare)                                                       # PrettyTable автоматически выравнивает столбцы и рисует рамки.


plt.figure(figsize=(8,5))                                            # создаём новый график (canvas) с размерами 8×5 дюймов.
plt.bar(["M1","M2","M3"], [r2_1, r2_2, r2_3],                       # рисуем столбчатую диаграмму:
        color=["#6fa8dc","#93c47d","#f6b26b"])                      # задаём три цвета для столбцов.
plt.title("📊 Сравнение точности моделей (R²)")                      # добавляем заголовок графика.
plt.ylabel("R² (на тесте)")                                          # подпись оси Y.

for i, v in enumerate([r2_1, r2_2, r2_3]):                           # цикл проходит по индексам и значениям R².
    plt.text(i, v+0.005, f"{v:.3f}", ha='center', fontweight='bold') # рисуем подписи над каждым столбцом (высота = v+0.005).
plt.tight_layout()                                                   # автоматически подгоняет отступы, чтобы всё влезло.
plt.show()                                                           # отображает график в окне / ноутбуке.

y1_true, y2_true, y3_true = y_test, y_test, y3_test                  # копируем массивы истинных значений, чтобы не путаться при итерации.
fig, axs = plt.subplots(1, 3, figsize=(16, 5))                       # создаём полотно (Figure) с 3 подграфиками (axes) в 1 строке.
                                                                    # axs — это массив из трёх осей (AxesSubplot).

models = [                                                           # список с параметрами каждой модели для цикла.
    ("M1: median_income", y1_true, y1_te, "#6fa8dc"),                # каждый элемент = (заголовок, факты, предсказания, цвет).
    ("M2: geo + age + income", y2_true, y2_te, "#93c47d"),
    ("M3: full + engineered", y3_true, y3_te, "#f6b26b")
]

for ax, (title, y_t, y_h, color) in zip(axs, models):                # проходим одновременно по осям и параметрам моделей.
    ax.scatter(y_t, y_h, s=10, color=color, alpha=0.6)               # строим scatter-график: фактические y_t (по X-оси) против предсказанных y_h (по Y-оси).
                                                                    # s=10 — размер точки, alpha=0.6 — прозрачность.

    lims = [min(y_t.min(), y_h.min()), max(y_t.max(), y_h.max())]    # определяем границы графика (минимум и максимум по обеим осям),
                                                                    # чтобы линия "y=x" шла по диагонали всего диапазона.

    ax.plot(lims, lims, 'r--', lw=1)                                 # рисуем пунктирную красную диагональ (идеальные предсказания: ŷ = y).
    ax.set_title(title, fontsize=11, fontweight='bold')              # заголовок над каждым подграфиком.
    ax.set_xlabel("Факт")                                            # подпись оси X.
    ax.set_ylabel("Предсказание")                                    # подпись оси Y.
    ax.grid(alpha=0.3)                                               # включаем сетку с прозрачностью 30%.

plt.suptitle("📈 Сравнение моделей: Факт vs Предсказание", fontsize=14, fontweight='bold')  
                                                                    # общий заголовок для всей фигуры (suptitle = super title).
plt.tight_layout(rect=[0, 0.03, 1, 0.95])                            # делаем отступ под заголовок.
plt.show()                                                           # отображаем все 3 scatter-графика в одном окне.

fig, axs = plt.subplots(1, 3, figsize=(16, 4))                       # снова создаём 3 подграфика для трёх моделей.

residuals = [                                                        # собираем список остатков (ошибок) для каждой модели.
    ("M1", y1_true - y1_te, "#6fa8dc"),                              # остатки = y - ŷ.
    ("M2", y2_true - y2_te, "#93c47d"),
    ("M3", y3_true - y3_te, "#f6b26b")
]

for ax, (name, res, color) in zip(axs, residuals):                   # перебираем все модели и оси графиков.
    ax.hist(res, bins=40, color=color, edgecolor='black', alpha=0.8) # строим гистограмму ошибок.
                                                                    # bins=40 — разбиваем диапазон остатков на 40 "корзин".
                                                                    # edgecolor='black' — чёрные границы столбцов.

    ax.set_title(f"Остатки — {name}", fontweight='bold')             # подпись над графиком.
    ax.set_xlabel("Ошибка (y - ŷ)")                                  # подпись оси X (отклонения).
    ax.set_ylabel("Частота")                                         # подпись оси Y.
    ax.grid(alpha=0.3)                                               # добавляем сетку для удобства чтения.

plt.suptitle("📉 Распределения ошибок по моделям", fontsize=14, fontweight='bold')  # общий заголовок.
plt.tight_layout(rect=[0, 0.03, 1, 0.95])                            # отступ под заголовок.
plt.show()                                                           # отображаем все 3 гистограммы.

best_model = max([(r2_1, "M1", y_test, y1_te),                      # создаём список кортежей (r2, имя, y_true, y_pred).
                  (r2_2, "M2", y_test, y2_te),
                  (r2_3, "M3", y3_test, y3_te)], key=lambda x: x[0])
                                                                    # max() ищет кортеж с максимальным первым элементом (r2).
                                                                    # key=lambda x: x[0] означает "сравнивать по R²".
                                                                    # возвращает кортеж лучшей модели, например (0.73, 'M3', y3_test, y3_te).
_, best_name, y_true, y_hat = best_model                             # распаковываем кортеж: имя модели, фактические и предсказанные значения.


plt.figure(figsize=(6,6))                                            # создаём новый график (квадратный формат 6×6).
plt.scatter(y_true, y_hat, s=10, color='teal')                       # scatter-график: фактические против предсказанных.
lims = [min(y_true.min(), y_hat.min()), max(y_true.max(), y_hat.max())]  # вычисляем границы осей (по минимальному и максимальному значению).
plt.plot(lims, lims, 'r--')                                          # рисуем красную пунктирную диагональ y=x.
plt.xlabel("Факт")                                                   # подпись оси X.
plt.ylabel("Предсказание")                                           # подпись оси Y.
plt.title(f"Факт vs Предсказание — {best_name}")                     # заголовок с именем модели.
plt.tight_layout()                                                   # подгоняем отступы.
plt.show()                                                           # отображаем график.

residuals = y_true - y_hat                                           # вычисляем ошибки предсказаний: разница между факт и предсказанием.
plt.figure(figsize=(8,5))                                            # создаём новое окно графика.
plt.hist(residuals, bins=40, edgecolor='black', color='salmon')      # строим гистограмму остатков.
plt.title(f"Распределение остатков — {best_name}")                   # заголовок с именем модели.
plt.xlabel("Остаток (y - ŷ)")                                        # подпись оси X.
plt.ylabel("Частота")                                                # подпись оси Y.
plt.tight_layout()                                                   # подгоняем расположение.
plt.show()                                                           # показываем график.
